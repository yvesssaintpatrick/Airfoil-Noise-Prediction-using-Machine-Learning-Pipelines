{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce8a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark==3.1.2 -q\n",
    "%pip install findspark -q\n",
    "\n",
    "# Suppress warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import necessary modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Airfoil Noise\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv(\"NASA_airfoil_noise_raw.csv\", header=True, inferSchema=True)\n",
    "\n",
    "#Print top 5 rows of the dataset\n",
    "df.show(5)\n",
    "\n",
    "#Print the total number of rows in the dataset\n",
    "rowcount1 = df.count()\n",
    "print(rowcount1)\n",
    "\n",
    "#Drop all the duplicate rows from the dataset\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "#Print the total number of rows in the dataset\n",
    "rowcount2 = df.count()\n",
    "print(rowcount2)\n",
    "\n",
    "#Drop all the rows that contain null values from the dataset\n",
    "df = df.na.drop()\n",
    "\n",
    "#Print the total number of rows in the dataset\n",
    "rowcount3 = df.count()\n",
    "print(rowcount3)\n",
    "\n",
    "#Rename the column \"SoundLevel\" to \"SoundLevelDecibels\"\n",
    "df = df.withColumnRenamed(\"SoundLevel\", \"SoundLevelDecibels\")\n",
    "\n",
    "#Save the dataframe in parquet format\n",
    "df.write.parquet(\"NASA_airfoil_noise_cleaned.parquet\")\n",
    "\n",
    "# Part 1 - Evaluation\n",
    "print(\"Part 1 - Evaluation\")\n",
    "print(\"Total rows = \", rowcount1)\n",
    "print(\"Total rows after dropping duplicate rows = \", rowcount2)\n",
    "print(\"Total rows after dropping duplicate rows and rows with null values = \", rowcount3)\n",
    "print(\"New column name = \", df.columns[-1])\n",
    "import os\n",
    "print(\"NASA_airfoil_noise_cleaned.parquet exists :\", os.path.isdir(\"NASA_airfoil_noise_cleaned.parquet\"))\n",
    "\n",
    "# Part 2 - Create a Machine Learning Pipeline\n",
    "\n",
    "# Load data from \"NASA_airfoil_noise_cleaned.parquet\"\n",
    "df = spark.read.parquet(\"NASA_airfoil_noise_cleaned.parquet\")\n",
    "\n",
    "#Print the total number of rows in the dataset\n",
    "rowcount4 = df.count()\n",
    "print(rowcount4)\n",
    "\n",
    "#Define the VectorAssembler pipeline stage\n",
    "assembler = VectorAssembler(inputCols=[col for col in df.columns if col != \"SoundLevelDecibels\"], outputCol=\"features\")\n",
    "\n",
    "#Define the StandardScaler pipeline stage\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "#Define the Model creation pipeline stage\n",
    "lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"SoundLevelDecibels\")\n",
    "\n",
    "#Build the pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, lr])\n",
    "\n",
    "#Split the data\n",
    "(trainingData, testingData) = df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "#Fit the pipeline\n",
    "pipelineModel = pipeline.fit(trainingData)\n",
    "\n",
    "# Part 2 - Evaluation\n",
    "print(\"Part 2 - Evaluation\")\n",
    "print(\"Total rows = \", rowcount4)\n",
    "ps = [str(x).split(\"_\")[0] for x in pipeline.getStages()]\n",
    "print(\"Pipeline Stage 1 = \", ps[0])\n",
    "print(\"Pipeline Stage 2 = \", ps[1])\n",
    "print(\"Pipeline Stage 3 = \", ps[2])\n",
    "print(\"Label column = \", lr.getLabelCol())\n",
    "\n",
    "# Part 3 - Evaluate the Model\n",
    "\n",
    "#Predict using the model\n",
    "predictions = pipelineModel.transform(testingData)\n",
    "\n",
    "#Print the MSE\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"SoundLevelDecibels\", predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(mse)\n",
    "\n",
    "#Print the MAE\n",
    "evaluator = RegressionEvaluator(labelCol=\"SoundLevelDecibels\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "mae = evaluator.evaluate(predictions)\n",
    "print(mae)\n",
    "\n",
    "#Print the R-Squared (R2)\n",
    "evaluator = RegressionEvaluator(labelCol=\"SoundLevelDecibels\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(r2)\n",
    "\n",
    "# Part 3 - Evaluation\n",
    "print(\"Part 3 - Evaluation\")\n",
    "print(\"Mean Squared Error = \", round(mse,2))\n",
    "print(\"Mean Absolute Error = \", round(mae,2))\n",
    "print(\"R Squared = \", round(r2,2))\n",
    "lrModel = pipelineModel.stages[-1]\n",
    "print(\"Intercept = \", round(lrModel.intercept,2))\n",
    "\n",
    "# Part 4 - Persist the Model\n",
    "\n",
    "#Save the model to the path \"Project\"\n",
    "pipelineModel.save(\"Project\")\n",
    "\n",
    "#Load the model from the path \"Project\"\n",
    "loadedPipelineModel = PipelineModel.load(\"Project\")\n",
    "\n",
    "#Make predictions using the loaded model on the test data\n",
    "predictions = loadedPipelineModel.transform(testingData)\n",
    "\n",
    "#Show the predictions\n",
    "predictions.select(\"SoundLevelDecibels\", \"prediction\").show(5)\n",
    "\n",
    "# Part 4 - Evaluation\n",
    "print(\"Part 4 - Evaluation\")\n",
    "loadedmodel = loadedPipelineModel.stages[-1]\n",
    "totalstages = len(loadedPipelineModel.stages)\n",
    "inputcolumns = loadedPipelineModel.stages[0].getInputCols()\n",
    "print(\"Number of stages in the pipeline = \", totalstages)\n",
    "for i, j in zip(inputcolumns, loadedmodel.coefficients):\n",
    "    print(f\"Coefficient for {i} is {round(j,4)}\")\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
